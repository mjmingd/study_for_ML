{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression(선형회귀)  \n",
    "주어진 x와 y 사이의 상관관계를 파악하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0)) #random_uniform : -1.0 ~ 1.0 사이의 무작위 값을 뽑아 균등분포로 만듦\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = W * X  + b #행렬 곱이 아니므로 * 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function(손실함수)\n",
    "- loss(손실값) : 실제값과 모델이 예측한 값의 차이  \n",
    "(손실값이 작을 수록 모델이 상관관계를 정확히 설명하고 있음을 의미)\n",
    "- cost(비용) : 손실값을 전체 데이터에 구한 경우\n",
    "- 학습은 손실값을 최소화하는 방향으로 진행됨(다양한 손실함수는 www.tensorflow.org/api_docs에서 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer(최적화)\n",
    "- W(가중치)와 b(편향값)을 변경해가면서 loss를 최소화하는 방법\n",
    "- 다양한 최적화 방법이 있지만 여기서는 gradient descent(경사하강법) 사용  \n",
    "    eg) gradient descent(경사하강법) : 가장 기본적인 알고리즘으로 함수의 기울기를 구하고 기울기의 반대 방향(기울기가 낮아지는 쪽)으로 이동\n",
    "- learning rate(학습률) : 최적화 함수의 매개변수로, 학습을 얼마나 빠르게 할 것인가를 결정함  lr이 너무 크면 loss의 minimum을 찾지 못함.  lr이 너무 작으면 학습 속도가 너무 느려짐.  \n",
    "    learning rate처럼 학습이 아니라 사람이 결정해주는 파라미터들을 hyperparameter(하이퍼 파라미터)라고 하며, hyperparameter를 튜닝하는 하는 것이 모델의 성능에 큰 영향을 끼침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.689567 [0.5495922] [1.2673472]\n",
      "1 0.26959017 [0.4630339] [1.1940409]\n",
      "2 0.2066478 [0.4865859] [1.1700191]\n",
      "3 0.19623296 [0.4977647] [1.1413809]\n",
      "4 0.18690462 [0.50996524] [1.1139989]\n",
      "5 0.1780264 [0.5217315] [1.087213]\n",
      "6 0.16957004 [0.5332302] [1.0610778]\n",
      "7 0.16151534 [0.5444509] [1.0355701]\n",
      "8 0.15384322 [0.555402] [1.0106758]\n",
      "9 0.14653556 [0.5660898] [0.98637986]\n",
      "10 0.13957508 [0.57652074] [0.962668]\n",
      "11 0.13294514 [0.58670086] [0.9395261]\n",
      "12 0.12663019 [0.5966363] [0.9169405]\n",
      "13 0.1206151 [0.6063329] [0.8948979]\n",
      "14 0.1148858 [0.6157964] [0.87338513]\n",
      "15 0.109428614 [0.62503237] [0.8523895]\n",
      "16 0.10423068 [0.6340464] [0.8318987]\n",
      "17 0.09927962 [0.6428436] [0.8119004]\n",
      "18 0.094563775 [0.6514294] [0.79238284]\n",
      "19 0.09007192 [0.6598088] [0.7733345]\n",
      "20 0.085793495 [0.66798675] [0.75474405]\n",
      "21 0.08171822 [0.67596817] [0.7366006]\n",
      "22 0.07783655 [0.6837576] [0.7188932]\n",
      "23 0.07413927 [0.69135994] [0.7016115]\n",
      "24 0.07061761 [0.6987794] [0.68474525]\n",
      "25 0.06726317 [0.70602053] [0.6682844]\n",
      "26 0.06406816 [0.7130876] [0.6522193]\n",
      "27 0.061024815 [0.71998477] [0.6365404]\n",
      "28 0.058126125 [0.72671616] [0.6212384]\n",
      "29 0.055365056 [0.73328567] [0.6063042]\n",
      "30 0.0527352 [0.7396974] [0.5917291]\n",
      "31 0.050230235 [0.7459549] [0.57750434]\n",
      "32 0.04784426 [0.7520619] [0.5636215]\n",
      "33 0.045571607 [0.7580222] [0.5500725]\n",
      "34 0.04340698 [0.7638392] [0.53684914]\n",
      "35 0.04134507 [0.7695163] [0.52394366]\n",
      "36 0.03938115 [0.77505696] [0.5113484]\n",
      "37 0.037510507 [0.7804644] [0.49905595]\n",
      "38 0.035728768 [0.7857419] [0.487059]\n",
      "39 0.034031585 [0.7908925] [0.4753504]\n",
      "40 0.032415096 [0.79591936] [0.46392334]\n",
      "41 0.030875353 [0.80082536] [0.45277095]\n",
      "42 0.029408744 [0.80561334] [0.4418866]\n",
      "43 0.02801179 [0.8102862] [0.43126395]\n",
      "44 0.026681239 [0.8148468] [0.42089668]\n",
      "45 0.02541384 [0.81929773] [0.4107786]\n",
      "46 0.024206681 [0.8236418] [0.40090382]\n",
      "47 0.023056844 [0.8278813] [0.39126635]\n",
      "48 0.021961637 [0.83201885] [0.38186055]\n",
      "49 0.020918421 [0.83605707] [0.3726809]\n",
      "50 0.019924795 [0.8399981] [0.3637219]\n",
      "51 0.018978363 [0.8438445] [0.3549783]\n",
      "52 0.018076846 [0.8475983] [0.34644485]\n",
      "53 0.017218195 [0.851262] [0.33811656]\n",
      "54 0.016400317 [0.8548375] [0.32998845]\n",
      "55 0.015621279 [0.8583271] [0.32205576]\n",
      "56 0.014879267 [0.86173284] [0.31431377]\n",
      "57 0.01417249 [0.8650567] [0.3067579]\n",
      "58 0.013499285 [0.8683006] [0.29938364]\n",
      "59 0.012858075 [0.8714666] [0.29218665]\n",
      "60 0.012247295 [0.8745564] [0.2851627]\n",
      "61 0.011665546 [0.87757206] [0.2783076]\n",
      "62 0.011111411 [0.8805151] [0.27161723]\n",
      "63 0.010583612 [0.88338745] [0.26508775]\n",
      "64 0.010080888 [0.8861907] [0.2587152]\n",
      "65 0.009602038 [0.8889266] [0.25249588]\n",
      "66 0.009145935 [0.89159673] [0.24642605]\n",
      "67 0.008711493 [0.8942027] [0.24050215]\n",
      "68 0.00829769 [0.896746] [0.23472065]\n",
      "69 0.007903539 [0.8992281] [0.22907813]\n",
      "70 0.0075281295 [0.90165067] [0.22357127]\n",
      "71 0.0071705324 [0.9040149] [0.21819675]\n",
      "72 0.0068299216 [0.90632224] [0.21295142]\n",
      "73 0.0065054987 [0.9085742] [0.20783225]\n",
      "74 0.0061964733 [0.910772] [0.2028361]\n",
      "75 0.005902152 [0.912917] [0.19796006]\n",
      "76 0.0056217895 [0.91501045] [0.19320124]\n",
      "77 0.005354753 [0.9170535] [0.1885568]\n",
      "78 0.0051003923 [0.9190475] [0.18402402]\n",
      "79 0.004858114 [0.9209935] [0.17960021]\n",
      "80 0.004627357 [0.9228928] [0.17528278]\n",
      "81 0.0044075544 [0.92474645] [0.17106912]\n",
      "82 0.004198192 [0.92655545] [0.16695672]\n",
      "83 0.0039987783 [0.928321] [0.1629432]\n",
      "84 0.003808833 [0.9300441] [0.15902616]\n",
      "85 0.0036279168 [0.9317258] [0.15520328]\n",
      "86 0.0034555774 [0.9333671] [0.1514723]\n",
      "87 0.0032914337 [0.9349689] [0.14783101]\n",
      "88 0.0031350984 [0.9365322] [0.14427724]\n",
      "89 0.002986174 [0.9380579] [0.14080891]\n",
      "90 0.0028443316 [0.939547] [0.13742398]\n",
      "91 0.0027092183 [0.9410002] [0.13412039]\n",
      "92 0.0025805268 [0.9424185] [0.13089621]\n",
      "93 0.0024579484 [0.9438027] [0.12774956]\n",
      "94 0.0023412067 [0.9451537] [0.12467857]\n",
      "95 0.0022299916 [0.9464721] [0.12168137]\n",
      "96 0.0021240646 [0.9477589] [0.11875625]\n",
      "97 0.0020231714 [0.9490147] [0.11590143]\n",
      "98 0.0019270641 [0.9502404] [0.11311525]\n",
      "99 0.001835533 [0.9514366] [0.11039604]\n",
      "X: 5, Y:  [4.867579]\n",
      "X: 2.5, Y:  [2.4889877]\n"
     ]
    }
   ],
   "source": [
    "#with 구문을 써서 session이 종료될때까지 자동으로 코드가 돌아가도록\n",
    "with tf.Session() as sess : \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #Train은 100번\n",
    "    for step in range(100): \n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "        \n",
    "    #Test : train에 없던 값을 넣어서 확인!     \n",
    "    print(\"X: 5, Y: \", sess.run(hypothesis, feed_dict= {X : 5}))\n",
    "    print(\"X: 2.5, Y: \", sess.run(hypothesis, feed_dict= {X: 2.5}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
