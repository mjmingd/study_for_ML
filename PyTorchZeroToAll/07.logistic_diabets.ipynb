{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare Data\n",
    "Load diabets file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([759, 8])\n",
      "torch.Size([759, 1])\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('../PyTorchZeroToAll/data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = Variable(torch.from_numpy(xy[:, :-1]))\n",
    "y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n",
    "\n",
    "print(x_data.data.shape)  #[the number of data, the number of features]\n",
    "print(y_data.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "Logistic Regression with 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)  ## the size of input featrues is 8 \n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Construct Loss function and Optimizer\n",
    "* Loss function : BCE Loss\n",
    "* Optimizer : SGD\n",
    "* Learning rate : 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train \n",
    "* epoch : 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.6696)\n",
      "1 tensor(0.6673)\n",
      "2 tensor(0.6652)\n",
      "3 tensor(0.6633)\n",
      "4 tensor(0.6615)\n",
      "5 tensor(0.6600)\n",
      "6 tensor(0.6586)\n",
      "7 tensor(0.6574)\n",
      "8 tensor(0.6562)\n",
      "9 tensor(0.6552)\n",
      "10 tensor(0.6543)\n",
      "11 tensor(0.6535)\n",
      "12 tensor(0.6528)\n",
      "13 tensor(0.6521)\n",
      "14 tensor(0.6515)\n",
      "15 tensor(0.6509)\n",
      "16 tensor(0.6504)\n",
      "17 tensor(0.6500)\n",
      "18 tensor(0.6496)\n",
      "19 tensor(0.6492)\n",
      "20 tensor(0.6489)\n",
      "21 tensor(0.6486)\n",
      "22 tensor(0.6483)\n",
      "23 tensor(0.6481)\n",
      "24 tensor(0.6479)\n",
      "25 tensor(0.6477)\n",
      "26 tensor(0.6475)\n",
      "27 tensor(0.6474)\n",
      "28 tensor(0.6472)\n",
      "29 tensor(0.6471)\n",
      "30 tensor(0.6470)\n",
      "31 tensor(0.6468)\n",
      "32 tensor(0.6467)\n",
      "33 tensor(0.6467)\n",
      "34 tensor(0.6466)\n",
      "35 tensor(0.6465)\n",
      "36 tensor(0.6464)\n",
      "37 tensor(0.6464)\n",
      "38 tensor(0.6463)\n",
      "39 tensor(0.6463)\n",
      "40 tensor(0.6462)\n",
      "41 tensor(0.6462)\n",
      "42 tensor(0.6462)\n",
      "43 tensor(0.6461)\n",
      "44 tensor(0.6461)\n",
      "45 tensor(0.6461)\n",
      "46 tensor(0.6460)\n",
      "47 tensor(0.6460)\n",
      "48 tensor(0.6460)\n",
      "49 tensor(0.6460)\n",
      "50 tensor(0.6460)\n",
      "51 tensor(0.6459)\n",
      "52 tensor(0.6459)\n",
      "53 tensor(0.6459)\n",
      "54 tensor(0.6459)\n",
      "55 tensor(0.6459)\n",
      "56 tensor(0.6459)\n",
      "57 tensor(0.6459)\n",
      "58 tensor(0.6459)\n",
      "59 tensor(0.6459)\n",
      "60 tensor(0.6458)\n",
      "61 tensor(0.6458)\n",
      "62 tensor(0.6458)\n",
      "63 tensor(0.6458)\n",
      "64 tensor(0.6458)\n",
      "65 tensor(0.6458)\n",
      "66 tensor(0.6458)\n",
      "67 tensor(0.6458)\n",
      "68 tensor(0.6458)\n",
      "69 tensor(0.6458)\n",
      "70 tensor(0.6458)\n",
      "71 tensor(0.6458)\n",
      "72 tensor(0.6458)\n",
      "73 tensor(0.6458)\n",
      "74 tensor(0.6458)\n",
      "75 tensor(0.6458)\n",
      "76 tensor(0.6458)\n",
      "77 tensor(0.6458)\n",
      "78 tensor(0.6458)\n",
      "79 tensor(0.6458)\n",
      "80 tensor(0.6458)\n",
      "81 tensor(0.6458)\n",
      "82 tensor(0.6458)\n",
      "83 tensor(0.6458)\n",
      "84 tensor(0.6458)\n",
      "85 tensor(0.6458)\n",
      "86 tensor(0.6458)\n",
      "87 tensor(0.6457)\n",
      "88 tensor(0.6457)\n",
      "89 tensor(0.6457)\n",
      "90 tensor(0.6457)\n",
      "91 tensor(0.6457)\n",
      "92 tensor(0.6457)\n",
      "93 tensor(0.6457)\n",
      "94 tensor(0.6457)\n",
      "95 tensor(0.6457)\n",
      "96 tensor(0.6457)\n",
      "97 tensor(0.6457)\n",
      "98 tensor(0.6457)\n",
      "99 tensor(0.6457)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data)\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
